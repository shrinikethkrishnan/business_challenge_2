{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008656,
     "end_time": "2023-02-21T04:32:23.015311",
     "exception": false,
     "start_time": "2023-02-21T04:32:23.006655",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00448,
     "end_time": "2023-02-21T04:32:23.024665",
     "exception": false,
     "start_time": "2023-02-21T04:32:23.020185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Below is a starter code that comes with Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.023977,
     "end_time": "2023-02-21T04:32:23.053296",
     "exception": false,
     "start_time": "2023-02-21T04:32:23.029319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004442,
     "end_time": "2023-02-21T04:32:23.062733",
     "exception": false,
     "start_time": "2023-02-21T04:32:23.058291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br><br>\n",
    "You will need to import any additional libraries. It's also a good idea to get rid of the cell above once you're used to Jupyter Notebook in Kaggle.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 1.341598,
     "end_time": "2023-02-21T04:32:24.409041",
     "exception": false,
     "start_time": "2023-02-21T04:32:23.067443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modeling library\n",
    "import sklearn.linear_model                          # linear modeling in scikit-learn\n",
    "\n",
    "# other model building tools\n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.metrics import roc_auc_score            # auc score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005058,
     "end_time": "2023-02-21T04:32:24.419616",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.414558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br><br>\n",
    "Make sure to tell Python the path to your training data. If you have no idea what a path is, look to the left and find and hover over <em>Data&nbsp;>&nbsp;Input&nbsp;>&nbsp;spaceship-titanic</em>. A button should appear allowing you to copy the path to your clipboard.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.072113,
     "end_time": "2023-02-21T04:32:24.496482",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.424369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   8693 non-null   object \n",
      " 1   HomePlanet    8492 non-null   object \n",
      " 2   CryoSleep     8476 non-null   object \n",
      " 3   Cabin         8494 non-null   object \n",
      " 4   Destination   8511 non-null   object \n",
      " 5   Age           8514 non-null   float64\n",
      " 6   VIP           8490 non-null   object \n",
      " 7   RoomService   8512 non-null   float64\n",
      " 8   FoodCourt     8510 non-null   float64\n",
      " 9   ShoppingMall  8485 non-null   float64\n",
      " 10  Spa           8510 non-null   float64\n",
      " 11  VRDeck        8505 non-null   float64\n",
      " 12  Name          8493 non-null   object \n",
      " 13  Transported   8693 non-null   bool   \n",
      "dtypes: bool(1), float64(6), object(7)\n",
      "memory usage: 891.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# importing the training dataset\n",
    "path             = \"./train.csv\"\n",
    "training_dataset = \"train.csv\"\n",
    "\n",
    "\n",
    "# reading in the .csv file with pandas\n",
    "titanic_train    = pd.read_csv(path)\n",
    "\n",
    "\n",
    "# checking basic info about the dataset\n",
    "titanic_train.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004648,
     "end_time": "2023-02-21T04:32:24.506027",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.501379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "The test set is also available. Notice that there is no data for Transported (y-variable) in the test set. This is intentional.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.049335,
     "end_time": "2023-02-21T04:32:24.560334",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.510999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4277 entries, 0 to 4276\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   4277 non-null   object \n",
      " 1   HomePlanet    4190 non-null   object \n",
      " 2   CryoSleep     4184 non-null   object \n",
      " 3   Cabin         4177 non-null   object \n",
      " 4   Destination   4185 non-null   object \n",
      " 5   Age           4186 non-null   float64\n",
      " 6   VIP           4184 non-null   object \n",
      " 7   RoomService   4195 non-null   float64\n",
      " 8   FoodCourt     4171 non-null   float64\n",
      " 9   ShoppingMall  4179 non-null   float64\n",
      " 10  Spa           4176 non-null   float64\n",
      " 11  VRDeck        4197 non-null   float64\n",
      " 12  Name          4183 non-null   object \n",
      "dtypes: float64(6), object(7)\n",
      "memory usage: 434.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# importing the training dataset\n",
    "path             = \"test.csv\"\n",
    "testing_dataset  = 'test.csv'\n",
    "\n",
    "# importing the testing dataset\n",
    "titanic_test = pd.read_csv(path)\n",
    "\n",
    "# checking basic info about the dataset\n",
    "titanic_test.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "You may want to join the datasets together before feature engineering. This will help make things more efficient.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrin\\AppData\\Local\\Temp\\ipykernel_31420\\4111620901.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  titanic_df = titanic_train.append(other = titanic_test)\n"
     ]
    }
   ],
   "source": [
    "titanic_train['set'] = 'Training'\n",
    "titanic_test ['set'] = 'Testing'\n",
    "\n",
    "# concatenating both datasets together for mv and feature engineering\n",
    "titanic_df = titanic_train.append(other = titanic_test)\n",
    "\n",
    "# resetting index to avoid problems later in the code\n",
    "titanic_df.reset_index(drop = False,\n",
    "                       inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df[['deck','num', 'side']] = titanic_df['Cabin'].str.split('/', expand=True)\n",
    "#titanic[['deck','num', 'side']] = test_df['Cabin'].str.split('/', expand=True)\n",
    "\n",
    "titanic_df.drop('Cabin', axis=1, inplace=True)\n",
    "#test_df.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005017,
     "end_time": "2023-02-21T04:32:24.570678",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.565661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "Let's look at correlations to find an x-variable to model with.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 0.037642,
     "end_time": "2023-02-21T04:32:24.613658",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.576016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transported     1.00\n",
       "RoomService     0.24\n",
       "Spa             0.22\n",
       "VRDeck          0.21\n",
       "Age             0.08\n",
       "FoodCourt       0.05\n",
       "ShoppingMall    0.01\n",
       "Name: Transported, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a correlation matrix\n",
    "titanic_corr = titanic_train.corr(method = 'pearson').round(decimals = 2)\n",
    "\n",
    "# transforming correlations to absolute values\n",
    "titanic_corr.loc[ : , 'Transported' ].apply(func = abs).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00512,
     "end_time": "2023-02-21T04:32:24.624463",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.619343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "Since training and testing sets have already been developed for us, we can simply select the X-features we want to model with and we're good to go.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.015991,
     "end_time": "2023-02-21T04:32:24.645791",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.6298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imputing in missing values for RoomService\n",
    "titanic_df[ 'Destination' ].fillna(value = 0, inplace = True)\n",
    "\n",
    "# setting explanatory variable(s) with most correlated x-variable\n",
    "x_train = titanic_df[ ['Destination'] ][ titanic_df['set'] == 'Training' ]\n",
    "\n",
    "# setting response variable\n",
    "y_train = titanic_df[ 'Transported' ][ titanic_df['set']   == 'Training' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "Even though there's already a testing set, it's a good idea to build one on the training data to assess model performance and stability qualities.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# developing training and validation sets\n",
    "x_train_1, x_train_2, y_train_1, y_train_2 = train_test_split(\n",
    "            x_train,\n",
    "            y_train.astype(dtype = 'int'),\n",
    "            random_state = 123,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005209,
     "end_time": "2023-02-21T04:32:24.656603",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.651394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "Below is a Logistic Regression model using scikit-learn. Note that as you explore additional scikit-learn models, the only things that need to change are the model name and the model type.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.034158,
     "end_time": "2023-02-21T04:32:24.696287",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.662129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'TRAPPIST-1e'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31420\\87432367.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# FITTING to the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1506\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1508\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m   1509\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2064\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'TRAPPIST-1e'"
     ]
    }
   ],
   "source": [
    "# picking a model name\n",
    "model_name = \"Logistic Regression\"\n",
    "\n",
    "\n",
    "# INSTANTIATING a model object - CHANGE THIS AS NEEDED\n",
    "model = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "\n",
    "# FITTING to the training data\n",
    "model_fit = model.fit(x_train_1, y_train_1)\n",
    "\n",
    "\n",
    "# PREDICTING on the response variable\n",
    "model_train_pred = model_fit.predict(x_train_1)\n",
    "model_valid_pred = model_fit.predict(x_train_2)\n",
    "\n",
    "\n",
    "# SCORING the results (accuracy)\n",
    "model_train_score = model.score(x_train_1, y_train_1).round(4) # training accuracy\n",
    "model_valid_score = model.score(x_train_2, y_train_2).round(4) # validation accuracy\n",
    "\n",
    "# SCORING the results (auc)\n",
    "model_train_auc = roc_auc_score(y_true  = y_train_1,\n",
    "                                y_score = model_train_pred).round(decimals = 4)\n",
    "\n",
    "model_valid_auc = roc_auc_score(y_true  = y_train_2,\n",
    "                                y_score = model_valid_pred).round(decimals = 4)\n",
    "\n",
    "# displaying results\n",
    "print('Training Accuracy:  ', model_train_score)\n",
    "print('Validation Accuracy:', model_valid_score)\n",
    "print('Training AUC:       ', model_train_auc)\n",
    "print('Validation AUC:     ', model_valid_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005405,
     "end_time": "2023-02-21T04:32:24.707676",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.702271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Creating and Submitting Predictions</h2>\n",
    "In this competition, your results should be submitted as a .csv file. This file should contain exactly two columns:<br><br>\n",
    "\n",
    "1. Id (from the test set)\n",
    "2. SalePrice (predictions from your model)\n",
    "\n",
    "<br>\n",
    "The following code with do exactly that. Notice that all we need to do is apply the same X-variables (x_data) to the .predict step to generate our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.019818,
     "end_time": "2023-02-21T04:32:24.733143",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.713325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting x_test\n",
    "x_test  = titanic_df[ ['Destination'] ][ titanic_df['set'] == 'Testing' ]\n",
    "\n",
    "# PREDICTING on new data\n",
    "model_pred = model.predict(x_test)\n",
    "\n",
    "# checking results\n",
    "model_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005513,
     "end_time": "2023-02-21T04:32:24.744601",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.739088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "From here, we simply create a DataFrame with the original passenger ids (&nbsp;<em>PassengerId</em>&nbsp;) and the predicted values from the model (&nbsp;<em>Transported</em>&nbsp;).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025754,
     "end_time": "2023-02-21T04:32:24.776225",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.750471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving predictions with their respective Ids from the test set\n",
    "predictions = pd.DataFrame(data = { 'PassengerId' : titanic_test['PassengerId'],\n",
    "                                    'Transported' : model_pred.astype(bool)               } )\n",
    "\n",
    "# checking the results\n",
    "predictions.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005849,
     "end_time": "2023-02-21T04:32:24.788295",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.782446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "Finally, we save the DataFrame as a .csv file and we're ready to submit. This file will be available in the <em>/kaggle/working</em> directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021386,
     "end_time": "2023-02-21T04:32:24.815839",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.794453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sending predictions to .csv file\n",
    "predictions.to_csv(path_or_buf = './submission.csv',\n",
    "                   index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006064,
     "end_time": "2023-02-21T04:32:24.828411",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.822347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "~~~\n",
    "\n",
    "  _    _                            _____          _ _             _ \n",
    " | |  | |                          / ____|        | (_)           | |\n",
    " | |__| | __ _ _ __  _ __  _   _  | |     ___   __| |_ _ __   __ _| |\n",
    " |  __  |/ _` | '_ \\| '_ \\| | | | | |    / _ \\ / _` | | '_ \\ / _` | |\n",
    " | |  | | (_| | |_) | |_) | |_| | | |___| (_) | (_| | | | | | (_| |_|\n",
    " |_|  |_|\\__,_| .__/| .__/ \\__, |  \\_____\\___/ \\__,_|_|_| |_|\\__, (_)\n",
    "              | |   | |     __/ |                             __/ |  \n",
    "              |_|   |_|    |___/                             |___/   \n",
    "\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005897,
     "end_time": "2023-02-21T04:32:24.840559",
     "exception": false,
     "start_time": "2023-02-21T04:32:24.834662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
